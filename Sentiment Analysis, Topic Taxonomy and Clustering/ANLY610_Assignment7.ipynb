{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data=open(\"anly610_amazon.json\").readlines()\n",
    "feeds_read_from_file = []\n",
    "for line in json_data:\n",
    "    feeds_read_from_file.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_titles = []\n",
    "i = 0\n",
    "for record in feeds_read_from_file:\n",
    "#     record['id'] = i\n",
    "#     print(record['id'], str(record['title']))\n",
    "    i+=1\n",
    "    feed_titles.append(str(record['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_titles(title):\n",
    "    tokens = nltk.word_tokenize(title)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.replace(\"'s\", \" \").replace(\"n’t\", \" not\").replace(\"’ve\", \" have\")\n",
    "        token = re.sub(r'[^a-zA-Z0-9 ]', '', token)\n",
    "        if token not in stopwords:\n",
    "            filtered_tokens.append(token.lower())\n",
    "    \n",
    "    lemmas = [lmtzr.lemmatize(t,'v') for t in filtered_tokens]\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clstr_lda(num_topics, stories):\n",
    "    # top words to be identified\n",
    "    n_top_words = 10\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.05, min_df=0.001, max_features=200,\n",
    "                                    tokenizer=tokenize_titles, ngram_range=(3,4))\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(stories)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=300,\n",
    "                                    learning_method='online', learning_offset=10.,\n",
    "                                    random_state = 1)\n",
    "    lda.fit(tf)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # print top topic words\n",
    "    topics = dict()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topics[topic_idx] = [tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" | \".join([tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "   |     | amazon   |  1 trillion |   amazon |  amazon  |  allege bezos phone |  allege bezos | allege bezos phone hack | allege bezos phone\n",
      "Topic #1:\n",
      "jeff bezos   |  need know | whatsapp security flaw |   need know | know whatsapp security | know whatsapp security flaw | need know whatsapp security | need know whatsapp |  need know whatsapp |   need\n",
      "Topic #2:\n",
      "spend   |  39 spend | 39 spend  |  39 spend  | 39 spend   | prime  39 | prime  39 spend |   0 | delivery   |   0 prime\n",
      "Topic #3:\n",
      "keep phone safe hackers | phone safe hackers | keep phone safe | amazon prime video | nt like jeff |   keep phone safe | nt like jeff bezos |    keep phone | bezos    keep | bezos   \n",
      "Topic #4:\n",
      " 1 billion |  giveaway  | amazon prime video | amazon   | saudi crown prince | crown prince hack |  1 trillion |  business insider | amazon great indian | bezos  hack\n",
      "Topic #5:\n",
      " amazon  |  jeff bezos | new job india | jeff bezos phone | news  et |  business insider |   amazon | amazon prime video | jeff bezos phone hack |   jeff bezos\n",
      "Topic #6:\n",
      " 1 trillion | report hack jeff | report hack jeff bezos | probe report hack | urge probe report hack | probe report hack jeff |  phone saudi arabia | bezos  phone saudi |  phone saudi | phone saudi arabia\n",
      "Topic #7:\n",
      " news  |  amzn  | news  et |  1 billion | mohammed bin salman |  1 trillion | crown prince mohammed bin | prince mohammed bin | prince mohammed bin salman | crown prince mohammed\n",
      "Topic #8:\n",
      "battle top uk | remainers brexit battle | remainers brexit battle top | brexit battle top uk | top uk single | top uk single chart | leavers remainers brexit | leavers remainers brexit battle | uk single chart | brexit battle top\n",
      "Topic #9:\n",
      "  jeff bezos | amazon   jeff | amazon   jeff bezos |   jeff bezos  | planet  get | wealthiest person planet  | wealthiest person planet | person planet  | person planet  get | jeff bezos  already\n",
      "Topic #10:\n",
      "amid uncertainty virus | stock drop amid | stock drop amid uncertainty | drop amid uncertainty virus | drop amid uncertainty | amid uncertainty virus impact | uncertainty virus impact | virus impact  |  amazon soar | impact  amazon soar\n",
      "Topic #11:\n",
      "hack jeff bezos | hack jeff bezos  | bezos hack  | bezos  hack | crown prince hack | jeff bezos  phone | saudi crown prince | bezos  phone | mohammed bin salman | report hack jeff\n",
      "Topic #12:\n",
      "150 million prime | million prime members | 150 million prime members |  150 million |  1 trillion | spend   |  business insider |  giveaway  | amazon prime video | great indian sale\n",
      "Topic #13:\n",
      "bezos  phone | jeff bezos  phone |  phone hack | bezos  phone hack |  phone  | bezos  phone  | hack amazon boss | boss jeff bezos | amazon boss jeff bezos | amazon boss jeff\n",
      "Topic #14:\n",
      "crown prince  | saudi crown prince  | hack saudi crown | hack saudi crown prince | amazon great indian | great indian sale | phone hack saudi |  business insider | phone hack saudi crown | crown prince hack\n",
      "Topic #15:\n",
      "despite risk job | amazon climate despite | climate despite risk | climate despite risk job | amazon climate despite risk | criticize amazon climate despite | workers criticize amazon | criticize amazon climate | workers criticize amazon climate | amazon  \n",
      "Topic #16:\n",
      "  amazon |  amazon au |   amazon au | spend   amazon |  delivery  | delivery   | 39 spend   | 39 spend  |  39 spend |  39 spend \n",
      "Topic #17:\n",
      " really  | workforce hit 500k | really big  workforce | big  really | big  really  | big  workforce | big  workforce hit | amazon big  really | amazon big  |  workforce hit\n",
      "Topic #18:\n",
      "saudi crown prince |  1 billion | like jeff bezos  | like jeff bezos |   keep phone |    keep | start wag 500 | walmart test higher | test higher hourly start | hourly start wag\n",
      "Topic #19:\n",
      "bezos phone hack | jeff bezos phone | jeff bezos phone hack | ceo jeff bezos | amazon ceo jeff | amazon ceo jeff bezos | allege bezos phone | allege bezos phone hack |  allege bezos |  allege bezos phone\n"
     ]
    }
   ],
   "source": [
    "topics = clstr_lda(20, feed_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
